{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "267d4911-b4cf-48d7-bc4a-59caca45bca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.checkpoints;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a514dd-4ea6-4a10-a3ec-9b808f77d502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DROP VOLUME workspace.default.checkpoints;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "572ee5e5-4b4f-472f-b08c-3a02de27a4b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure AWS credentials for both Spark and boto3\n",
    "aws_access_key_id = \"AKIAWVRJWKZZBTQ4BHUI\"\n",
    "aws_secret_access_key = \"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\"\n",
    "\n",
    "# Set environment variables (works in both serverless and standard compute)\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key_id\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_access_key\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'ap-south-1'\n",
    "\n",
    "# Additional S3-specific environment variables for Spark\n",
    "os.environ['AWS_S3_ACCESS_KEY'] = aws_access_key_id\n",
    "os.environ['AWS_S3_SECRET_KEY'] = aws_secret_access_key\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed25e94-fe97-4b3c-9905-aab577f1b0b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# --- Configuration Widgets ---\n",
    "dbutils.widgets.text(\"s3_raw_bucket\", \"pk-transactions-raw\", \"S3 Raw Bucket\")\n",
    "dbutils.widgets.text(\"s3_detections_bucket\", \"pk-transactions-detections\", \"S3 Detections Bucket\")\n",
    "dbutils.widgets.text(\"s3_temp_bucket\", \"pk-transactions-temp\", \"S3 Temp Bucket\")\n",
    "\n",
    "dbutils.widgets.text(\"customer_importance_table\", \"workspace.default.customer_importance\", \"Customer Importance Table Name\")\n",
    "dbutils.widgets.text(\"postgres_host\", \"databricks-postgres-db.cd424ookkd9v.ap-south-1.rds.amazonaws.com\", \"Postgres Host\")\n",
    "dbutils.widgets.text(\"postgres_user\", \"postgres\", \"Postgres User\")\n",
    "dbutils.widgets.text(\"postgres_db\", \"postgres\", \"Postgres DB\")\n",
    "dbutils.widgets.text(\"postgres_password\", \"Abcabc123\", \"Postgres Password\")\n",
    "\n",
    "# --- Get Widget Values ---\n",
    "s3_raw_path = f\"s3a://{dbutils.widgets.get('s3_raw_bucket')}\"\n",
    "s3_detections_path = f\"s3a://{dbutils.widgets.get('s3_detections_bucket')}\"\n",
    "customer_importance_table_name = dbutils.widgets.get('customer_importance_table')\n",
    "postgres_host = dbutils.widgets.get('postgres_host')\n",
    "postgres_user = dbutils.widgets.get('postgres_user')\n",
    "postgres_db = dbutils.widgets.get('postgres_db')\n",
    "postgres_password = dbutils.widgets.get('postgres_password')\n",
    "\n",
    "# --- Checkpoint Location on a Unity Catalog Volume ---\n",
    "volume_checkpoint_path = \"/Volumes/workspace/default/checkpoints/mechanism_y\"\n",
    "s3_checkpoint_location = volume_checkpoint_path\n",
    "\n",
    "# Ensure the directory exists. This is idempotent.\n",
    "dbutils.fs.mkdirs(s3_checkpoint_location)\n",
    "\n",
    "# --- JDBC Connection Properties ---\n",
    "jdbc_url = f\"jdbc:postgresql://{postgres_host}:5432/{postgres_db}\"\n",
    "connection_properties = { \"user\": postgres_user, \"password\": postgres_password, \"driver\": \"org.postgresql.Driver\" }\n",
    "\n",
    "# --- Define Transaction Schema from Source Table ---\n",
    "try:\n",
    "    source_table_for_schema = spark.table(\"workspace.default.transactions\")\n",
    "    transactions_schema = source_table_for_schema.schema\n",
    "    print(\"Successfully derived transactions schema from source table.\")\n",
    "except Exception as e:\n",
    "    raise Exception(\"Could not find source table 'workspace.default.transactions' to derive schema.\") from e\n",
    "\n",
    "print(\"\\nConfiguration complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb4099d-280a-4eaf-8766-7a0660615f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "import psycopg2\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import regexp_replace, trim, col, when, upper\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Global variable to store Y start time (set once when mechanism Y starts)\n",
    "Y_START_TIME = datetime.now(pytz.timezone('Asia/Kolkata')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def clean_quoted_strings(df, string_columns):\n",
    "    \"\"\"Remove single quotes from string columns\"\"\"\n",
    "    for column in string_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(column, regexp_replace(col(column), \"^'|'$\", \"\"))\n",
    "    return df\n",
    "\n",
    "def normalize_gender(df, gender_column=\"gender\"):\n",
    "    \"\"\"Normalize gender values to standard format\"\"\"\n",
    "    return df.withColumn(\n",
    "        gender_column,\n",
    "        when(upper(col(gender_column)).isin(['M', 'MALE', '1']), 'M')\n",
    "        .when(upper(col(gender_column)).isin(['F', 'FEMALE', '2']), 'F')\n",
    "        .otherwise('Unknown')  # Unknown/Other\n",
    "    )\n",
    "\n",
    "def create_empty_detections_df():\n",
    "    \"\"\"Create empty DataFrame with proper schema for detections\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"YStartTime_IST\", StringType(), True),\n",
    "        StructField(\"detectionTime_IST\", StringType(), True),\n",
    "        StructField(\"patternId\", StringType(), True),\n",
    "        StructField(\"ActionType\", StringType(), True),\n",
    "        StructField(\"customerName\", StringType(), True),\n",
    "        StructField(\"MerchantId\", StringType(), True)\n",
    "    ])\n",
    "    return spark.createDataFrame([], schema)\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    print(f\"--- Processing Batch ID: {batch_id} ---\")\n",
    "    if batch_df.isEmpty():\n",
    "        print(\"Empty batch - skipping processing\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(\"=== Data Cleaning and Validation ===\")\n",
    "        \n",
    "        # **Critical Fix 1: Clean quoted strings from transaction data**\n",
    "        string_columns = [\"customer\", \"merchant\", \"category\", \"gender\", \"zipcodeOri\", \"zipMerchant\"]\n",
    "        batch_df = clean_quoted_strings(batch_df, string_columns)\n",
    "        \n",
    "        # **Critical Fix 2: Normalize gender values and handle data types**\n",
    "        batch_df = normalize_gender(batch_df)\n",
    "        batch_df = batch_df.withColumn(\"age\", col(\"age\").cast(\"integer\"))\n",
    "        \n",
    "        # Data quality validation\n",
    "        print(f\"Batch records: {batch_df.count()}\")\n",
    "        gender_values = [row['gender'] for row in batch_df.select('gender').distinct().collect()]\n",
    "        print(f\"Gender values after cleaning: {gender_values}\")\n",
    "        \n",
    "        # **Critical Fix 3: Clean importance data with same logic**\n",
    "        importance_df = (\n",
    "            spark.table(customer_importance_table_name)\n",
    "              .withColumnRenamed(\"Source\", \"customer\")\n",
    "              .withColumnRenamed(\"Target\", \"merchant\")\n",
    "              .withColumnRenamed(\"Weight\", \"weight\")\n",
    "              .withColumnRenamed(\"typeTrans\", \"category\")\n",
    "              .select(\"customer\", \"merchant\", \"weight\", \"category\")\n",
    "        )\n",
    "        \n",
    "        # Clean quoted strings from importance data\n",
    "        importance_string_columns = [\"customer\", \"merchant\", \"category\"]\n",
    "        importance_df = clean_quoted_strings(importance_df, importance_string_columns)\n",
    "        \n",
    "        print(\"=== Database Operations ===\")\n",
    "        \n",
    "        # **Critical Fix 4: Improved connection management with error handling**\n",
    "        conn = None\n",
    "        cursor = None\n",
    "        \n",
    "        try:\n",
    "            conn = psycopg2.connect(\n",
    "                host=postgres_host, \n",
    "                dbname=postgres_db, \n",
    "                user=postgres_user, \n",
    "                password=postgres_password,\n",
    "                connect_timeout=10\n",
    "            )\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # --- Part 1: Update Merchant Counts in Postgres ---\n",
    "            merchant_counts_in_batch = batch_df.groupBy(\"merchant\").count()\n",
    "            merchant_updates = merchant_counts_in_batch.collect()\n",
    "            \n",
    "            print(f\"Updating merchant counts for {len(merchant_updates)} merchants...\")\n",
    "            for row in merchant_updates:\n",
    "                cursor.execute(\n",
    "                    \"\"\"INSERT INTO merchant_transaction_counts (merchant_id, transaction_count) \n",
    "                       VALUES (%s, %s) \n",
    "                       ON CONFLICT (merchant_id) \n",
    "                       DO UPDATE SET transaction_count = merchant_transaction_counts.transaction_count + EXCLUDED.transaction_count;\"\"\",\n",
    "                    (row['merchant'], row['count'])\n",
    "                )\n",
    "            \n",
    "            # --- Part 2: Update Customer-Merchant Statistics ---\n",
    "            batch_with_importance = batch_df.join(importance_df, [\"customer\", \"merchant\", \"category\"], \"inner\")\n",
    "            \n",
    "            if not batch_with_importance.isEmpty():\n",
    "                customer_merchant_batch_stats = batch_with_importance.groupBy(\"customer\", \"merchant\").agg(\n",
    "                    F.count(\"*\").alias(\"batch_txn_count\"),\n",
    "                    F.avg(\"weight\").alias(\"batch_avg_weight\"),\n",
    "                    F.sum(\"amount\").alias(\"batch_amount_sum\"),\n",
    "                    F.count(\"amount\").alias(\"batch_amount_count\")\n",
    "                )\n",
    "                \n",
    "                customer_stats_updates = customer_merchant_batch_stats.collect()\n",
    "                print(f\"Updating customer-merchant stats for {len(customer_stats_updates)} combinations...\")\n",
    "                \n",
    "                for row in customer_stats_updates:\n",
    "                    # **Critical Fix 5: Handle potential null values**\n",
    "                    weight_sum = float(row['batch_avg_weight'] or 0) * row['batch_txn_count']\n",
    "                    cursor.execute(\n",
    "                        \"\"\"INSERT INTO customer_merchant_stats (customer_id, merchant_id, transaction_count, weight_sum, weight_count, amount_sum, amount_count)\n",
    "                           VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                           ON CONFLICT (customer_id, merchant_id)\n",
    "                           DO UPDATE SET \n",
    "                               transaction_count = customer_merchant_stats.transaction_count + EXCLUDED.transaction_count,\n",
    "                               weight_sum = customer_merchant_stats.weight_sum + EXCLUDED.weight_sum,\n",
    "                               weight_count = customer_merchant_stats.weight_count + EXCLUDED.weight_count,\n",
    "                               amount_sum = customer_merchant_stats.amount_sum + EXCLUDED.amount_sum,\n",
    "                               amount_count = customer_merchant_stats.amount_count + EXCLUDED.amount_count;\"\"\",\n",
    "                        (row['customer'], row['merchant'], row['batch_txn_count'], \n",
    "                         weight_sum, row['batch_txn_count'],\n",
    "                         float(row['batch_amount_sum'] or 0), row['batch_amount_count'])\n",
    "                    )\n",
    "            else:\n",
    "                print(\"No matching records found between batch and importance data\")\n",
    "            \n",
    "            # --- Part 3: Update Customer Gender Stats per Merchant ---\n",
    "            gender_batch_stats = batch_df.groupBy(\"merchant\", \"gender\").agg(\n",
    "                F.countDistinct(\"customer\").alias(\"distinct_customers\")\n",
    "            )\n",
    "            \n",
    "            gender_updates = gender_batch_stats.collect()\n",
    "            print(f\"Updating gender stats for {len(gender_updates)} merchant-gender combinations...\")\n",
    "            \n",
    "            for row in gender_updates:\n",
    "                cursor.execute(\n",
    "                    \"\"\"INSERT INTO merchant_gender_stats (merchant_id, gender, customer_count)\n",
    "                       VALUES (%s, %s, %s)\n",
    "                       ON CONFLICT (merchant_id, gender)\n",
    "                       DO UPDATE SET customer_count = merchant_gender_stats.customer_count + EXCLUDED.customer_count;\"\"\",\n",
    "                    (row['merchant'], row['gender'], row['distinct_customers'])\n",
    "                )\n",
    "            \n",
    "            conn.commit()\n",
    "            print(\"Database updates committed successfully\")\n",
    "            \n",
    "        except psycopg2.Error as db_error:\n",
    "            print(f\"Database error: {db_error}\")\n",
    "            if conn:\n",
    "                conn.rollback()\n",
    "            raise\n",
    "            \n",
    "        finally:\n",
    "            if cursor:\n",
    "                cursor.close()\n",
    "            if conn:\n",
    "                conn.close()\n",
    "        \n",
    "        print(\"=== Pattern Detection ===\")\n",
    "        \n",
    "        # --- Pattern Detection Logic ---\n",
    "        y_start_time = F.lit(Y_START_TIME)\n",
    "        detection_time = F.lit(datetime.now(pytz.timezone('Asia/Kolkata')).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        \n",
    "        # Initialize empty detection DataFrames\n",
    "        patid1_detections = create_empty_detections_df()\n",
    "        patid2_detections = create_empty_detections_df()\n",
    "        patid3_detections = create_empty_detections_df()\n",
    "        \n",
    "        # **PatId1: Top 10% transaction count + Bottom 10% weight + Merchant >50K transactions**\n",
    "        try:\n",
    "            print(\"Running PatId1 detection...\")\n",
    "            eligible_merchants_df = spark.read.jdbc(\n",
    "                url=jdbc_url, \n",
    "                table=\"merchant_transaction_counts\", \n",
    "                properties=connection_properties\n",
    "            ).filter(F.col(\"transaction_count\") > 50000)\n",
    "            \n",
    "            if not eligible_merchants_df.isEmpty():\n",
    "                print(f\"Found {eligible_merchants_df.count()} eligible merchants for PatId1\")\n",
    "                \n",
    "                customer_stats_df = spark.read.jdbc(\n",
    "                    url=jdbc_url,\n",
    "                    table=\"(SELECT customer_id, merchant_id, transaction_count, CASE WHEN weight_count > 0 THEN weight_sum/weight_count ELSE 0 END as avg_weight FROM customer_merchant_stats WHERE weight_count > 0) as stats\",\n",
    "                    properties=connection_properties\n",
    "                )\n",
    "                \n",
    "                if not customer_stats_df.isEmpty():\n",
    "                    # Calculate percentiles per merchant\n",
    "                    merchant_window = Window.partitionBy(\"merchant_id\")\n",
    "                    percentiles_df = customer_stats_df.withColumn(\n",
    "                        \"txn_percentile\", F.percent_rank().over(merchant_window.orderBy(\"transaction_count\"))\n",
    "                    ).withColumn(\n",
    "                        \"weight_percentile\", F.percent_rank().over(merchant_window.orderBy(\"avg_weight\"))\n",
    "                    )\n",
    "                    \n",
    "                    patid1_detections = percentiles_df.join(\n",
    "                        eligible_merchants_df, \n",
    "                        percentiles_df.merchant_id == eligible_merchants_df.merchant_id\n",
    "                    ).filter(\n",
    "                        (F.col(\"txn_percentile\") >= 0.90) & (F.col(\"weight_percentile\") <= 0.10)\n",
    "                    ).select(\n",
    "                        y_start_time.alias(\"YStartTime_IST\"),\n",
    "                        detection_time.alias(\"detectionTime_IST\"),\n",
    "                        F.lit(\"PatId1\").alias(\"patternId\"),\n",
    "                        F.lit(\"UPGRADE\").alias(\"ActionType\"),\n",
    "                        F.col(\"customer_id\").alias(\"customerName\"),\n",
    "                        F.col(\"merchant_id\").alias(\"MerchantId\")\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"PatId1 detections: {patid1_detections.count()}\")\n",
    "                else:\n",
    "                    print(\"No customer stats available for PatId1\")\n",
    "            else:\n",
    "                print(\"No eligible merchants for PatId1 (>50K transactions)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"PatId1 detection error: {e}\")\n",
    "            patid1_detections = create_empty_detections_df()\n",
    "        \n",
    "        # **PatId2: Avg transaction <23 AND >=80 transactions**\n",
    "        try:\n",
    "            print(\"Running PatId2 detection...\")\n",
    "            customer_stats_df = spark.read.jdbc(\n",
    "                url=jdbc_url,\n",
    "                table=\"(SELECT customer_id, merchant_id, transaction_count, CASE WHEN amount_count > 0 THEN amount_sum/amount_count ELSE 0 END as avg_txn_value FROM customer_merchant_stats WHERE amount_count > 0) as stats\",\n",
    "                properties=connection_properties\n",
    "            )\n",
    "            \n",
    "            if not customer_stats_df.isEmpty():\n",
    "                patid2_detections = customer_stats_df.filter(\n",
    "                    (F.col(\"avg_txn_value\") < 23) & (F.col(\"transaction_count\") >= 80)\n",
    "                ).select(\n",
    "                    y_start_time.alias(\"YStartTime_IST\"),\n",
    "                    detection_time.alias(\"detectionTime_IST\"),\n",
    "                    F.lit(\"PatId2\").alias(\"patternId\"),\n",
    "                    F.lit(\"CHILD\").alias(\"ActionType\"),\n",
    "                    F.col(\"customer_id\").alias(\"customerName\"),\n",
    "                    F.col(\"merchant_id\").alias(\"MerchantId\")\n",
    "                )\n",
    "                \n",
    "                print(f\"PatId2 detections: {patid2_detections.count()}\")\n",
    "            else:\n",
    "                print(\"No customer stats available for PatId2\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"PatId2 detection error: {e}\")\n",
    "            patid2_detections = create_empty_detections_df()\n",
    "        \n",
    "        # **PatId3: Female < Male customers AND Female >100**\n",
    "        try:\n",
    "            print(\"Running PatId3 detection...\")\n",
    "            gender_pivot_query = \"\"\"\n",
    "            SELECT merchant_id,\n",
    "                   COALESCE(SUM(CASE WHEN gender = 'F' THEN customer_count END), 0) as female_customers,\n",
    "                   COALESCE(SUM(CASE WHEN gender = 'M' THEN customer_count END), 0) as male_customers\n",
    "            FROM merchant_gender_stats \n",
    "            GROUP BY merchant_id\n",
    "            HAVING COALESCE(SUM(CASE WHEN gender = 'F' THEN customer_count END), 0) > 0\n",
    "            \"\"\"\n",
    "            \n",
    "            gender_stats_df = spark.read.jdbc(\n",
    "                url=jdbc_url,\n",
    "                table=f\"({gender_pivot_query}) as gender_stats\",\n",
    "                properties=connection_properties\n",
    "            )\n",
    "            \n",
    "            if not gender_stats_df.isEmpty():\n",
    "                patid3_detections = gender_stats_df.filter(\n",
    "                    (F.col(\"female_customers\") < F.col(\"male_customers\")) & \n",
    "                    (F.col(\"female_customers\") > 100)\n",
    "                ).select(\n",
    "                    y_start_time.alias(\"YStartTime_IST\"),\n",
    "                    detection_time.alias(\"detectionTime_IST\"),\n",
    "                    F.lit(\"PatId3\").alias(\"patternId\"),\n",
    "                    F.lit(\"DEI-NEEDED\").alias(\"ActionType\"),\n",
    "                    F.lit(\"\").alias(\"customerName\"),\n",
    "                    F.col(\"merchant_id\").alias(\"MerchantId\")\n",
    "                )\n",
    "                \n",
    "                print(f\"PatId3 detections: {patid3_detections.count()}\")\n",
    "            else:\n",
    "                print(\"No gender stats available for PatId3\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"PatId3 detection error: {e}\")\n",
    "            patid3_detections = create_empty_detections_df()\n",
    "        \n",
    "        # **Critical Fix 6: Proper detection output handling with schema alignment**\n",
    "        all_detections = patid1_detections.unionByName(patid2_detections).unionByName(patid3_detections)\n",
    "        \n",
    "        if not all_detections.isEmpty():\n",
    "            detection_count = all_detections.count()\n",
    "            print(f\"Found {detection_count} total detections in this batch.\")\n",
    "            \n",
    "            # **Fix column names to match assignment requirements**\n",
    "            final_detections = all_detections.withColumnRenamed(\"YStartTime_IST\", \"YStartTime(IST)\") \\\n",
    "                                           .withColumnRenamed(\"detectionTime_IST\", \"detectionTime(IST)\")\n",
    "            \n",
    "            if detection_count > 0:\n",
    "                # Calculate number of partitions needed (each partition = max 50 records)\n",
    "                num_partitions = max(1, (detection_count + 49) // 50)  # Ceiling division\n",
    "                \n",
    "                final_detections.repartition(num_partitions).write.mode(\"append\").format(\"csv\").option(\"header\", \"true\").save(s3_detections_path)\n",
    "                print(f\"Written {detection_count} detections in {num_partitions} files\")\n",
    "        else:\n",
    "            print(\"No detections found in this batch\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in process_batch: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "    \n",
    "    print(f\"--- Completed Batch ID: {batch_id} ---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6c8b8f-be70-4a69-ab80-536e74f68698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")  \n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", s3_checkpoint_location)\n",
    "    .schema(transactions_schema)\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(s3_raw_path)\n",
    ")\n",
    "\n",
    "streaming_query = (\n",
    "    df_stream.writeStream\n",
    "    .foreachBatch(process_batch)\n",
    "    .option(\"checkpointLocation\", s3_checkpoint_location)\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"Streaming query started successfully.\")\n",
    "streaming_query.awaitTermination()\n",
    "print(\"Stream processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7546695059235419,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "mechanism_y_processor",
   "widgets": {
    "customer_importance": {
     "currentValue": "workspace.default.customer_importance",
     "nuid": "bbaaff24-a981-4a71-aa42-7b253a05c673",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace.default.customer_importance",
      "label": "Customer Importance Table Name",
      "name": "customer_importance",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace.default.customer_importance",
      "label": "Customer Importance Table Name",
      "name": "customer_importance",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "customer_importance_table": {
     "currentValue": "workspace.default.customer_importance",
     "nuid": "b3794e9f-f282-4a50-8ad7-513f5a15ce08",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace.default.customer_importance",
      "label": "Customer Importance Table Name",
      "name": "customer_importance_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace.default.customer_importance",
      "label": "Customer Importance Table Name",
      "name": "customer_importance_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "postgres_db": {
     "currentValue": "postgres",
     "nuid": "8c0e69d6-b601-4790-855d-4289808c136e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "postgres",
      "label": "Postgres DB",
      "name": "postgres_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "postgres",
      "label": "Postgres DB",
      "name": "postgres_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "postgres_host": {
     "currentValue": "databricks-postgres-db.cd424ookkd9v.ap-south-1.rds.amazonaws.com",
     "nuid": "c6e6e614-92ea-4ca6-9667-d8a06e72ff82",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-postgres-db.cd424ookkd9v.ap-south-1.rds.amazonaws.com",
      "label": "Postgres Host",
      "name": "postgres_host",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-postgres-db.cd424ookkd9v.ap-south-1.rds.amazonaws.com",
      "label": "Postgres Host",
      "name": "postgres_host",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "postgres_password": {
     "currentValue": "Abcabc123",
     "nuid": "da15600e-3108-4345-9b5d-460d2296b433",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Abcabc123",
      "label": "Postgres Password",
      "name": "postgres_password",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Abcabc123",
      "label": "Postgres Password",
      "name": "postgres_password",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "postgres_user": {
     "currentValue": "postgres",
     "nuid": "2e19ce3a-ce47-4a97-8c5f-f9f57bc53fbe",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "postgres",
      "label": "Postgres User",
      "name": "postgres_user",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "postgres",
      "label": "Postgres User",
      "name": "postgres_user",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "s3_detections_bucket": {
     "currentValue": "pk-transactions-detections",
     "nuid": "5f083a2e-848e-4774-a890-7037da40e049",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "pk-transactions-detections",
      "label": "S3 Detections Bucket",
      "name": "s3_detections_bucket",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "pk-transactions-detections",
      "label": "S3 Detections Bucket",
      "name": "s3_detections_bucket",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "s3_raw_bucket": {
     "currentValue": "pk-transactions-raw",
     "nuid": "7732ad95-e781-49be-a413-6945455ae36d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "pk-transactions-raw",
      "label": "S3 Raw Bucket",
      "name": "s3_raw_bucket",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "pk-transactions-raw",
      "label": "S3 Raw Bucket",
      "name": "s3_raw_bucket",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "s3_temp_bucket": {
     "currentValue": "pk-transactions-temp",
     "nuid": "5a5a7a78-5269-40fb-b355-d1980e5014e3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "pk-transactions-temp",
      "label": "S3 Temp Bucket",
      "name": "s3_temp_bucket",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "pk-transactions-temp",
      "label": "S3 Temp Bucket",
      "name": "s3_temp_bucket",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}